{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #1 - Prediction\n",
    "\n",
    "Welcome to the first exercise of this course. In this and the following exercises we will implement a (small) neural network that can predict the probability of a single condition being true or false, also known as binary classification.\n",
    "\n",
    "In this first exercise we will implement the required functions to build a neural network and test their correctness with artificial data. In the next exercises this neural network will be evaluated and trained with real data.\n",
    "\n",
    "## Imports\n",
    "\n",
    "Before we can start we need to include `NumPy`, the main Python library that we are going to use. This library _\"adds support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\"_ ([source](https://en.wikipedia.org/w/index.php?title=NumPy&oldid=1174952220)).\n",
    "\n",
    "To include it execute the next cell by selecting it and either click the `Run` button in the toolbar or press `CTRL+ENTER` on your keyboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense\n",
    "\n",
    "Let's build some basic building blocks which we can use to construct the neural network.\n",
    "\n",
    "A neural network consists of multiple layers. A layer $n$ first computes an intermediate value $z_n$ using the output of the previous layer ($a_{n-1}$) and the weights ($w_n$) and bias ($b_n$) of this layer. Remember that we want to implement the following function.\n",
    "\n",
    "$$z_n=a_{n-1} w_n + b_n$$\n",
    "\n",
    "We will call this function `dense` because a layer that is implemented like this is also called a \"dense\" or \"fully connected\" layer. It uses all its input values, i.e. it is densely connected.\n",
    "\n",
    "Implement this function in the code below, between the two commented lines.\n",
    "\n",
    "**Note**:\n",
    "The parameters and variables represent the following:\n",
    "\n",
    "- `a_prev`: the output of the previous layer, i.e. $a_{n-1}$\n",
    "- `w`: the weights of this layer, i.e. $w_n$\n",
    "- `b`: the bias is this layer, i.e. $b_n$\n",
    "- `z`: the intermediate output (before activation) of this layer, i.e. $z_n$\n",
    "\n",
    "**Hint**:\n",
    "For multiplying matrices you should use the [np.matmul](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html) function instead of the `*` operator. The latter is mapped to the [np.multiply](https://numpy.org/doc/stable/reference/generated/numpy.multiply.html) function that performs an element-wise multiplication, which is _not_ what we want. However, the addition of the bias value should be done element-wise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense(a_prev, w, b):\n",
    "    #### BEGIN IMPLEMENTATION ####\n",
    "    z = ...\n",
    "    #### END IMPLEMENTATION ####\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if you implemented it correctly by feeding it 2 samples of a previous layer that has 4 output units (shape `(2,4)`) and a weight/bias combination with shapes `(4,3)` and `(1,3)` respectively. This should result in 2 samples with 3 units (shape `(2,3)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_prev = np.array([[0.7443503, 0.25197198, 0.07746765, -0.04006432], [0.82262378, -0.88750386, -0.36685496, 0.84961117]])\n",
    "w = np.array([[-0.23438933, -0.20918998,  0.38962773],\n",
    "              [ 0.05811497, -0.4372891 ,  0.4132518 ],\n",
    "              [-0.00410555, -0.25582833, -0.0910004 ],\n",
    "              [ 0.33127268,  0.43464842,  0.40134338]])\n",
    "b = np.array([[0.56929805, -0.53694105, -0.37223993]])\n",
    "\n",
    "print(dense(a_prev, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be equal to \n",
    "\n",
    "    [[ 0.39588336 -0.84006859 -0.00122167]\n",
    "     [ 0.60786566  0.14220411 -0.04411569]]\n",
    "\n",
    "If this is not the case, then check your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations\n",
    "\n",
    "Next up are the activation functions. For our network we need the functions ReLU and Sigmoid.\n",
    "\n",
    "The ReLU activation function is defined as:\n",
    "\n",
    "$$g(z) = \\begin{cases} z, & z \\gt 0 \\\\ 0, & z \\le 0 \\end{cases}$$\n",
    "\n",
    "And the sigmoid function is defined as:\n",
    "\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "**Hint** You might need the functions [np.maximum](https://numpy.org/doc/stable/reference/generated/numpy.maximum.html) and [np.exp](https://numpy.org/doc/stable/reference/generated/numpy.exp.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    #### BEGIN IMPLEMENTATION ####\n",
    "    a = ...\n",
    "    #### END IMPLEMENTATION ####\n",
    "    return a\n",
    "\n",
    "def sigmoid(z):\n",
    "    #### BEGIN IMPLEMENTATION ####\n",
    "    a = ...\n",
    "    #### END IMPLEMENTATION ####\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if you implemented them correctly by feeding them the output of the previous `dense` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array([[0.39588336, -0.84006859, -0.00122167], [0.60786566, 0.14220411, -0.04411569]])\n",
    "print(sigmoid(z))\n",
    "print(relu(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be equal to\n",
    "\n",
    "    [[0.59769819 0.30152034 0.49969458]\n",
    "     [0.64745378 0.53549124 0.48897287]]\n",
    "    [[0.39588336 0.         0.        ]\n",
    "     [0.60786566 0.14220411 0.        ]]\n",
    "\n",
    "If this is not the case, then check your implementation before you continue to the next part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "With these functions we can now build our neural network, which is also called a model in frameworks like TensorFlow. We will create a model with 3 layers. Two \"hidden\" layers both with 64 units that use the ReLU activation function and an output layer with 1 unit that uses the Sigmoid activation function.\n",
    "\n",
    "![model architecture](figures/model.png \"Model architecture\")\n",
    "\n",
    "The Sigmoid function in the output layer will make sure the (single) output value is always between 0 and 1, representing the probability of the condition being `true` (i.e. prob >= 0.5) or `false`.\n",
    "\n",
    "The model is built as a class with a constructor to initialize the parameters and a single `predict` function to compute the output value.\n",
    "\n",
    "#### Constructor\n",
    "During initialization of the model (i.e. in the constructor) the weights and biases of all layers must be initialized. The weights must be initialized with a matrix (of the right shape) with uniform random values, uniformly distributed within **-0.5** and **+0.5**. The biases must be initialized with only zeros.\n",
    "\n",
    "You will need the functions [np.zeros](https://numpy.org/doc/stable/reference/generated/numpy.zeros.html) and [np.random.uniform](https://numpy.org/doc/stable/reference/random/generated/numpy.random.uniform.html). Be aware that `np.zeros` requires a single parameter, a tuple, to define the size (i.e. shape) of the tensor. You have to add extra parentheses, like for example `np.zeros((4,3))`.\n",
    "\n",
    "**Note**: The variables `N0` till `N3` represent the number of units in each layer, where `N0` is the number of input units and `N3` the number of output units of the whole network.\n",
    "\n",
    "#### Predict\n",
    "\n",
    "In the `predict` function you should stack up all the computations for the network so it results in a single prediction.\n",
    "\n",
    "Here you should use the previously implemented `dense`, `relu` and `sigmoid` functions.\n",
    "\n",
    "**Hint**: Make sure you have the shapes correct. If you are unsure then you can debug this using lines like `print(a2.shape)` to print the shapes. And remember the matrix multiplications rule $(n,k) \\cdot (k,m) \\rightarrow (n,m)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        N0, N1, N2, N3 = 5, 64, 64, 1\n",
    "        #### BEGIN IMPLEMENTATION ####\n",
    "        self.w1 = ...\n",
    "        self.b1 = ...\n",
    "        self.w2 = ...\n",
    "        self.b2 = ...\n",
    "        self.w3 = ...\n",
    "        self.b3 = ...\n",
    "        #### END IMPLEMENTATION ####\n",
    "\n",
    "    def predict(self, x):\n",
    "        a0 = x\n",
    "        #### BEGIN IMPLEMENTATION ####\n",
    "        z1 = ...\n",
    "        a1 = ...\n",
    "        z2 = ...\n",
    "        a2 = ...\n",
    "        z3 = ...\n",
    "        a3 = ...\n",
    "        #### END IMPLEMENTATION ####\n",
    "        return a3        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if you implemented everything correctly by predicting a value for two samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siouxdnn import reset_seed\n",
    "x = np.array([[-0.64863997, -0.52876784,  0.18748115, -0.8999688 , -0.40311535],\n",
    "              [-0.58094129, -0.68657316, -0.46113119, -0.34206706,  0.08281399]])\n",
    "reset_seed()\n",
    "model = Model()\n",
    "y_pred = model.predict(x)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output should be equal to\n",
    "\n",
    "    [[0.28766463]\n",
    "     [0.42664955]]\n",
    "\n",
    "Check your implementation if this is not the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done\n",
    "\n",
    "This concludes the forward propagation part of the model. You have now implemented a neural network that can, given one or more input vectors, predict whether or not a certain condition is true. A binary classifier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
