{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #2 - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise you implemented a number of basic functions and combined them into a neural network model that can predict a value. But how can we judge if the output is any good? We need to evaluate the output by comparing it to a ground truth using a loss functions, which we will implement in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the NumPy library again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if a neural network is performing well we can use the loss function. This function should give a low value when the predictions of the network are close to the ground truth and a large value of they are not.\n",
    "\n",
    "In the previous exercise we have built a binary classifier model, so we are going to implement the **binary cross entropy loss** function to determine how well it fits to the ground truth. This function is defined as follows.\n",
    "\n",
    "$$\\mathop{BinaryCrossEntropyLoss}(y, \\hat{y}) = -{( y \\cdot \\log{\\hat{y}} + (1 - y) \\cdot \\log{(1 - \\hat{y})})}$$\n",
    "\n",
    "Where $y$ is our ground truth and $\\hat{y}$ is the prediction. The following plot shows the behavior of the loss function in the two cases of the ground truth.\n",
    "\n",
    "![min log](figures/minlog.png \"-log\")\n",
    "\n",
    "In other words, if the ground truth is false (`y=0`) and we predict a value close to `0` then the loss will go down to zero. If we predict a value closer to `1` (i.e. the opposite of the ground truth), then the loss will go up to infinity.\n",
    "\n",
    "This loss value will be computed for every sample in the batch, but the optimization routine used during training expects a single scalar value. We can **reduce** the array of losses to a single scalar value by simply taking the average, which gives us the final loss value.\n",
    "\n",
    "$$\\mathop{L}(y, \\hat{y}) = \\frac{1}{N} \\sum_n^N \\mathop{BinaryCrossEntropyLoss}(\\hat{y}_n, y_n)$$\n",
    "\n",
    "Where $N$ is the number of samples in the batch.\n",
    "\n",
    "Let's implement them both in the function below.\n",
    "\n",
    "**Hint:**\n",
    "You will need [np.log](https://numpy.org/doc/stable/reference/generated/numpy.log.html) and [np.mean](https://numpy.org/doc/stable/reference/generated/numpy.mean.html).\n",
    "And remember that the `*` operator is an element-wise multiplication (mapped to [np.multiply](https://numpy.org/doc/stable/reference/generated/numpy.multiply.html)), which you _do_ need to use here instead of matrix multiplications.\n",
    "\n",
    "**Note:**\n",
    "* `y_true` represents the ground truth $y$\n",
    "* `y_pred` represent the predicted value(s), i.e. the output $\\hat{y}$ from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-9, 1-1e-9) # clip to prevent log of 0.\n",
    "    #### BEGIN IMPLEMENTATION ####\n",
    "    # Compute Binary Cross Entropy Loss for every sample\n",
    "    losses = ...\n",
    "    # Reduce to a single value\n",
    "    loss = ...\n",
    "    #### END IMPLEMENTATION ####\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for a test. If we provide this function with a prediction that is far from the ground truth it should give a relatively high loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([[1.0], [1.0], [0.0], [1.0], [0.0], [0.0]])\n",
    "y_pred = np.array([[0.1], [0.2], [0.7], [0.1], [0.9], [0.8]])\n",
    "loss = binary_cross_entropy_loss(y_true, y_pred)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be `1.8884339846960454`.\n",
    "\n",
    "But if we give it a prediction close to the ground truth the loss value should be low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([[1.0], [1.0], [0.0], [1.0], [0.0], [0.0]])\n",
    "y_pred = np.array([[0.9], [0.8], [0.3], [0.9], [0.1], [0.2]])\n",
    "loss = binary_cross_entropy_loss(y_true, y_pred)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be `0.18650726559010514`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Time to combine this with the model we created in the previous exercise.\n",
    "\n",
    "You should now create a function that first predicts a value for input `x` using the given model (with the `predict` function), and then computes the loss with respect to the ground truth (`y_true`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x, y_true):\n",
    "    #### BEGIN IMPLEMENTATION ####\n",
    "    y_pred = ...\n",
    "    loss = ...\n",
    "    #### END IMPLEMENTATION ####\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "To test your implementation we are going to use some real data. We are going to use this dataset throughout the rest of the exercises. The dataset that we are going to use is the [Breast Cancer Prediction Dataset](https://www.kaggle.com/merishnasuwal/breast-cancer-prediction-dataset) from the public Kaggle datasets library.\n",
    "\n",
    "> Worldwide, breast cancer is the most common type of cancer in women and the second highest in terms of mortality rates.Diagnosis of breast cancer is performed when an abnormal lump is found (from self-examination or x-ray) or a tiny speck of calcium is seen (on an x-ray). After a suspicious lump is found, the doctor will conduct a diagnosis to determine whether it is cancerous and, if so, whether it has spread to other parts of the body.\n",
    ">\n",
    "> This breast cancer dataset was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg.\n",
    "\n",
    "This dataset is a table of 569 rows and 6 columns. The first 5 columns are the results of certain measurements. The last column is the final diagnosis, where a `1` means that it is in fact a malignant tumor (i.e. a tumor that may invade its surrounding tissue or spread around the body) and a `0` means it is benign.\n",
    "\n",
    "So lets import the data set. The helper function splits this data into two sets, a training set and a validation set. This will come in handy later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siouxdnn import load_data\n",
    "X_train, Y_train, X_val, Y_val = load_data()\n",
    "print('training set', X_train.shape, Y_train.shape)\n",
    "print('validation set', X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "If all is implemented well, we should now be able to evaluate our previously implemented model. We use a reference implementation of `Model` that should behave exactly the same as your implementation. Call the `evaluate` function with the proper input from the dataset loaded above. Use the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siouxdnn import reset_seed, Model\n",
    "reset_seed()\n",
    "model = Model()\n",
    "#### BEGIN IMPLEMENTATION ####\n",
    "loss = ...\n",
    "#### END IMPLEMENTATION ####\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be `0.8438193071051783` otherwise check your implementation of `binary_cross_entropy_loss` or `evaluate`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done\n",
    "\n",
    "We are now done with this exercise. We can now quickly determine how well a model performs using a loss function. We will use this in the next exercise to actually train the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
