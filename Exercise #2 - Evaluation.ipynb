{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #2 - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise you implemented a number of basic functions and combined them into a neural network model that can predict a value. But how can we judge if the output is any good? We need to evaluate the output by comparing it to a ground truth using metric (and loss) functions, which we will implement in this exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First import the NumPy library again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: right;\" src=\"figures/precisionrecall.png\" width=\"315\" height=\"573\">\n",
    "\n",
    "## Metrics\n",
    "\n",
    "There are [several ways](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers) to check how well your model performs. We will first inspect some metrics.\n",
    "\n",
    "The most common metrics in use are:\n",
    "\n",
    "- *Binary accuracy*: How many of all the predictions are correct.\n",
    "\n",
    "        (tp + tn) / (tp + fp + tn + fn)\n",
    "\n",
    "- *Precision*: How many of the positive predictions are correct.\n",
    "\n",
    "        tp / (tp + fp)\n",
    "    \n",
    "- *Recall*: How many of the actual positive ground truths are predicted correct.\n",
    "\n",
    "        tp / (tp + fn)\n",
    "\n",
    "Implement the `get_metrics` function that, given the ground truth (`y_true`) and the predictions (`y_pred`), returns the accuracy, precision and recall values.\n",
    "\n",
    "**Note:** The variables `tp`, `fp`, `fn` and `fp` have already been computed for you. They are each integers representing the number of elements for that statistic. For example `fp` is the number of false positives.\n",
    "\n",
    "Be aware to prevent division by zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_true, y_pred):\n",
    "    y_true = (y_true >= 0.5)\n",
    "    y_pred = (y_pred >= 0.5)\n",
    "\n",
    "    tp = np.sum(y_pred & y_true)\n",
    "    fp = np.sum(y_pred & ~y_true)\n",
    "    fn = np.sum(~y_pred & y_true)\n",
    "    tn = np.sum(~y_pred & ~y_true)\n",
    "\n",
    "    #### BEGIN IMPLEMENTATION ####\n",
    "    accuracy = \n",
    "    precision = \n",
    "    recall = \n",
    "    #### END IMPLEMENTATION ####\n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the implementation. We give it 10 predictions which are not all correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([[1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0]])\n",
    "y_pred = np.array([[0.6], [0.4], [0.2], [0.7], [0.1], [0.2], [0.5], [0.9], [0.8], [0.6]])\n",
    "print(get_metrics(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be `(0.7, 0.6666666666666666, 0.8)`. In other words, an accuracy of 70%, a precision of 67%, and a recall of 80%.\n",
    "* 7 out of 10 predictions (70%) match the ground truth.\n",
    "* From the 6 elements that are predicted positive only 4 (67%) should have been predicted positive. So we have 2 false positives.\n",
    "* From the 5 elements that should have been predicted positive only 4 (80%) are actually predicted positive. One is missing, i.e. a false negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can also see what happens if your data is not well balanced. Say we have a data set with mainly negative samples and only a few positive samples. If we simply only predict a negative value (e.g. always output 0), then we get a very high accuracy (most samples are predicted correctly), but precision and recall will be terrible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([[1.0], [1.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0]])\n",
    "y_pred = np.zeros_like(y_true)\n",
    "print(get_metrics(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be `(0.8, 0.0, 0.0)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inversely, if we have a dataset with mainly positive samples and we always predict a positive value then both accuracy and precision will be high, and recall will be perfect. All seems right, but it actually isn't. So always pay attention to the distribution of your data, It should be well balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([[0.0], [0.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0]])\n",
    "y_pred = np.ones_like(y_true)\n",
    "print(get_metrics(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be `(0.8, 0.8, 1.0)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to check if a neural network is performing well (and more useful during training) is the loss function.\n",
    "\n",
    "We have built a binary classifier model, so we are going to implement the **binary cross entropy loss** function to determine how well it fits to the ground truth. This function is defined as follows.\n",
    "\n",
    "$$\\mathop{BinaryCrossEntropyLoss}(y, \\hat{y}) = -{( y \\cdot \\log{\\hat{y}} + (1 - y) \\cdot \\log{(1 - \\hat{y})})}$$\n",
    "\n",
    "Where $y$ is our ground truth and $\\hat{y}$ is the prediction. The following plot shows the the behavior of the loss function in the two cases of the ground truth.\n",
    "\n",
    "![min log](figures/minlog.png \"-log\")\n",
    "\n",
    "In other words, if the ground truth is a `1` and we predict a value closer to `1` then the loss will go down to zero. If we predict a value closer to `0` (i.e. the opposite of the ground truth), then the loss will go up to infinity.\n",
    "\n",
    "This loss value will be computed for every sample in the batch, but the optimization routine used during training expects a single scalar value. We can **reduce** the array of losses to a single scalar value by simply taking the average, which gives us the final loss value.\n",
    "\n",
    "$$\\mathop{L}(y, \\hat{y}) = \\frac{1}{N} \\sum_n^N \\mathop{BinaryCrossEntropyLoss}(\\hat{y}_n, y_n)$$\n",
    "\n",
    "Where $N$ is the number of samples in the batch.\n",
    "\n",
    "Let's implement them both in the function below.\n",
    "\n",
    "**Hint:**\n",
    "You will need [np.log](https://numpy.org/doc/stable/reference/generated/numpy.log.html) and [np.mean](https://numpy.org/doc/stable/reference/generated/numpy.mean.html).\n",
    "And remember that the `*` operator is an element-wise multiplication (mapped to [np.multiply](https://numpy.org/doc/stable/reference/generated/numpy.multiply.html)), which you _do_ need to use here instead of matrix multiplications.\n",
    "\n",
    "**Note:**\n",
    "* `y_true` represents the ground truth $y$\n",
    "* `y_pred` represent the predicted value(s), i.e. the output $\\hat{y}$ from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-9, 1-1e-9) # clip to prevent log of 0.\n",
    "    #### BEGIN IMPLEMENTATION ####\n",
    "    # Compute Binary Cross Entropy Loss for every sample\n",
    "    losses = \n",
    "    # Reduce to a single value\n",
    "    loss = \n",
    "    #### END IMPLEMENTATION ####\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time for a test. If we provide this function with a prediction that is far from the ground truth it should give a relatively high loss value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([[1.0], [1.0], [0.0], [1.0], [0.0], [0.0]])\n",
    "y_pred = np.array([[0.1], [0.2], [0.7], [0.1], [0.9], [0.8]])\n",
    "loss = binary_cross_entropy_loss(y_true, y_pred)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be `1.8884339846960454`.\n",
    "\n",
    "But if we give it a prediction close to the ground truth the loss value should be low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([[1.0], [1.0], [0.0], [1.0], [0.0], [0.0]])\n",
    "y_pred = np.array([[0.9], [0.8], [0.3], [0.9], [0.1], [0.2]])\n",
    "loss = binary_cross_entropy_loss(y_true, y_pred)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be `0.18650726559010514`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Time to combine this with the model we created in the previous exercise. You should now create a function that computes the loss of a model given some input (`x`) and the ground truth (`y_true`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, x, y_true):\n",
    "    #### BEGIN IMPLEMENTATION ####\n",
    "    y_pred = \n",
    "    loss = \n",
    "    #### END IMPLEMENTATION ####\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Let's import some real data to test your implementation. We are going to use this set throughout the rest of the exercises. The dataset that we are going to use is the [Breast Cancer Prediction Dataset](https://www.kaggle.com/merishnasuwal/breast-cancer-prediction-dataset) from the public Kaggle datasets library.\n",
    "\n",
    "> Worldwide, breast cancer is the most common type of cancer in women and the second highest in terms of mortality rates.Diagnosis of breast cancer is performed when an abnormal lump is found (from self-examination or x-ray) or a tiny speck of calcium is seen (on an x-ray). After a suspicious lump is found, the doctor will conduct a diagnosis to determine whether it is cancerous and, if so, whether it has spread to other parts of the body.\n",
    ">\n",
    "> This breast cancer dataset was obtained from the University of Wisconsin Hospitals, Madison from Dr. William H. Wolberg.\n",
    "\n",
    "This dataset is a table of 569 rows and 6 columns. The first 5 columns are the results of certain measurements. The last column is the final diagnosis, where a `1` means that it is in fact a malignant tumor (i.e. a tumor that may invade its surrounding tissue or spread around the body) and a `0` means it is benign.\n",
    "\n",
    "So lets import the data set. The helper function splits this data into two sets, a training set and a validation set. This will come in handy later on. For now we will only use the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siouxdnn import load_data\n",
    "X_train, Y_train, X_val, Y_val = load_data()\n",
    "print('training set', X_train.shape, Y_train.shape)\n",
    "print('validation set', X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test\n",
    "\n",
    "If all is implemented well, we should now be able to compute the loss of our previously implemented model on the validation dataset. We use a reference implementation of `Model` that is exactly the same as your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siouxdnn import reset_seed, Model\n",
    "reset_seed()\n",
    "model = Model()\n",
    "loss = evaluate(model, X_val, Y_val)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be `0.8438193071051783` otherwise check your implementation of `binary_cross_entropy_loss` or `evaluate`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done\n",
    "\n",
    "We are now done with exercise #2. We can now quickly determine how well a model performs using metrics and loss functions. We will use these in the next exercise to actually train the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
