{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(func, param):\n",
    "    epsilon = 1e-9\n",
    "    g = np.zeros_like(param)\n",
    "    \n",
    "    initial_value = func()\n",
    "    for i in range(param.shape[0]):\n",
    "        for j in range(param.shape[1]):\n",
    "            t = param[i, j]\n",
    "            param[i, j] = t + epsilon\n",
    "            value = func()\n",
    "            param[i, j] = t\n",
    "            g[i, j] = (value - initial_value) / epsilon\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = (np.random.uniform(0, 1, size=(20, 1)) >= 0.5)*1\n",
    "\n",
    "def check(d1, d2):\n",
    "    diff = d2 - d1\n",
    "    p2p = diff.max() - diff.min()\n",
    "    print(p2p)\n",
    "    if (p2p < 1e-6):\n",
    "        print('OK')\n",
    "    else:\n",
    "        print('Not OK')\n",
    "        print(np.hstack([d1, d2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siouxdnn import binary_cross_entropy_loss, binary_cross_entropy_loss_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_pred):\n",
    "    return binary_cross_entropy_loss(y_true, y_pred)\n",
    "\n",
    "def loss_backward(y_pred):\n",
    "    l = binary_cross_entropy_loss(y_true, y_pred)\n",
    "    dy = binary_cross_entropy_loss_backward(y_true, y_pred)\n",
    "    return dy\n",
    "\n",
    "y_pred = np.random.uniform(0, 1, size=(20,1))\n",
    "d1 = grad(lambda: loss(y_pred), y_pred)\n",
    "d2 = loss_backward(y_pred)\n",
    "check(d1, d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siouxdnn import sigmoid, sigmoid_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(z):\n",
    "    y_pred = sigmoid(z)\n",
    "    return binary_cross_entropy_loss(y_true, y_pred)\n",
    "\n",
    "def loss_backward(z):\n",
    "    y_pred = sigmoid(z)\n",
    "    l = binary_cross_entropy_loss(y_true, y_pred)\n",
    "    dy = binary_cross_entropy_loss_backward(y_true, y_pred)\n",
    "    dz = sigmoid_backward(z, dy)\n",
    "    return dz\n",
    "\n",
    "z = np.random.uniform(-1, 1, size=(20,1))\n",
    "d1 = grad(lambda: loss(z), z)\n",
    "d2 = loss_backward(z)\n",
    "check(d1, d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siouxdnn import relu, relu_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(z):\n",
    "    y_pred = relu(z)\n",
    "    return binary_cross_entropy_loss(y_true, y_pred)\n",
    "\n",
    "def loss_backward(z):\n",
    "    y_pred = relu(z)\n",
    "    l = binary_cross_entropy_loss(y_true, y_pred)\n",
    "    dy = binary_cross_entropy_loss_backward(y_true, y_pred)\n",
    "    dz = relu_backward(z, dy)\n",
    "    return dz\n",
    "\n",
    "z = np.random.uniform(-1, 1, size=(20,1))\n",
    "d1 = grad(lambda: loss(z), z)\n",
    "d2 = loss_backward(z)\n",
    "check(d1, d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siouxdnn import dense, dense_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(-1, 1, size=(20, 5))\n",
    "w = np.random.uniform(-1, 1, size=(5, 1))\n",
    "b = np.random.uniform(-1, 1, size=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense `a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x):\n",
    "    z = dense(x, w, b)\n",
    "    y_pred = sigmoid(z)\n",
    "    l = binary_cross_entropy_loss(y_true, y_pred)\n",
    "    return l\n",
    "\n",
    "def loss_backward(x):\n",
    "    z = dense(x, w, b)\n",
    "    y_pred = sigmoid(z)\n",
    "    l = binary_cross_entropy_loss(y_true, y_pred)\n",
    "    dy_pred = binary_cross_entropy_loss_backward(y_true, y_pred)\n",
    "    dz = sigmoid_backward(z, dy_pred)\n",
    "    dw, db, dx = dense_backward(x, w, dz)\n",
    "    return dx\n",
    "\n",
    "d1 = grad(lambda: loss(x), x)\n",
    "d2 = loss_backward(x)\n",
    "check(d1, d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense `w`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(w):\n",
    "    z = dense(x, w, b)\n",
    "    y_pred = sigmoid(z)\n",
    "    l = binary_cross_entropy_loss(y_true, y_pred)\n",
    "    return l\n",
    "\n",
    "def loss_backward(w):\n",
    "    z = dense(x, w, b)\n",
    "    y_pred = sigmoid(z)\n",
    "    l = binary_cross_entropy_loss(y_true, y_pred)\n",
    "    dy_pred = binary_cross_entropy_loss_backward(y_true, y_pred)\n",
    "    dz = sigmoid_backward(z, dy_pred)\n",
    "    dw, db, dx = dense_backward(x, w, dz)\n",
    "    return dw\n",
    "\n",
    "d1 = grad(lambda: loss(w), w)\n",
    "d2 = loss_backward(w)\n",
    "check(d1, d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense `b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(b):\n",
    "    z = dense(x, w, b)\n",
    "    y_pred = sigmoid(z)\n",
    "    l = binary_cross_entropy_loss(y_true, y_pred)\n",
    "    return l\n",
    "\n",
    "def loss_backward(b):\n",
    "    z = dense(x, w, b)\n",
    "    y_pred = sigmoid(z)\n",
    "    l = binary_cross_entropy_loss(y_true, y_pred)\n",
    "    dy_pred = binary_cross_entropy_loss_backward(y_true, y_pred)\n",
    "    dz = sigmoid_backward(z, dy_pred)\n",
    "    dw, db, dx = dense_backward(x, w, dz)\n",
    "    return db\n",
    "\n",
    "d1 = grad(lambda: loss(b), b)\n",
    "d2 = loss_backward(b)\n",
    "check(d1, d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(-1, 1, size=(20, 5))\n",
    "w1 = np.random.uniform(-1, 1, size=(5, 6))\n",
    "b1 = np.random.uniform(-1, 1, size=(1, 6))\n",
    "w2 = np.random.uniform(-1, 1, size=(6, 1))\n",
    "b2 = np.random.uniform(-1, 1, size=(1, 1))\n",
    "\n",
    "def loss(w1):\n",
    "    z1 = dense(x, w1, b1)\n",
    "    a1 = relu(z1)\n",
    "    z2 = dense(a1, w2, b2)\n",
    "    y_pred = sigmoid(z2)\n",
    "    l = binary_cross_entropy_loss(y_true, y_pred)\n",
    "    return l\n",
    "\n",
    "def loss_backward(w1):\n",
    "    z1 = dense(x, w1, b1)\n",
    "    a1 = relu(z1)\n",
    "    z2 = dense(a1, w2, b2)\n",
    "    y_pred = sigmoid(z2)\n",
    "    l = binary_cross_entropy_loss(y_true, y_pred)\n",
    "    dy_pred = binary_cross_entropy_loss_backward(y_true, y_pred)\n",
    "    dz2 = sigmoid_backward(z2, dy_pred)\n",
    "    dw2, db2, da1 = dense_backward(a1, w2, dz2)\n",
    "    dz1 = relu_backward(z1, da1)\n",
    "    dw1, db1, dx = dense_backward(x, w1, dz1)\n",
    "    return dw1\n",
    "\n",
    "d1 = grad(lambda: loss(w1), w1)\n",
    "d2 = loss_backward(w1)\n",
    "check(d1, d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(-1, 1, size=(20, 5))\n",
    "w1 = np.random.uniform(-1, 1, size=(5, 6))\n",
    "b1 = np.random.uniform(-1, 1, size=(1, 6))\n",
    "w2 = np.random.uniform(-1, 1, size=(6, 1))\n",
    "b2 = np.random.uniform(-1, 1, size=(1, 1))\n",
    "\n",
    "def loss(b1):\n",
    "    z1 = dense(x, w1, b1)\n",
    "    a1 = relu(z1)\n",
    "    z2 = dense(a1, w2, b2)\n",
    "    y_pred = sigmoid(z2)\n",
    "    l = binary_cross_entropy_loss(y_true, y_pred)\n",
    "    return l\n",
    "\n",
    "def loss_backward(b1):\n",
    "    z1 = dense(x, w1, b1)\n",
    "    a1 = relu(z1)\n",
    "    z2 = dense(a1, w2, b2)\n",
    "    y_pred = sigmoid(z2)\n",
    "    l = binary_cross_entropy_loss(y_true, y_pred)\n",
    "    dy_pred = binary_cross_entropy_loss_backward(y_true, y_pred)\n",
    "    dz2 = sigmoid_backward(z2, dy_pred)\n",
    "    dw2, db2, da1 = dense_backward(a1, w2, dz2)\n",
    "    dz1 = relu_backward(z1, da1)\n",
    "    dw1, db1, dx = dense_backward(x, w1, dz1)\n",
    "    return db1\n",
    "\n",
    "d1 = grad(lambda: loss(b1), b1)\n",
    "d2 = loss_backward(b1)\n",
    "check(d1, d2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
