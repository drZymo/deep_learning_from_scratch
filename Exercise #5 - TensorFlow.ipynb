{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #5 - TensorFlow\n",
    "\n",
    "Now that we know how a neural network computes a prediction and how it can be trained, we can look at how you can achieve this with a deep learning framework.\n",
    "\n",
    "We will take a look at [TensorFlow](https://www.tensorflow.org/) and we will use it to create a neural network model that can classify images with handwritten digits (0-9).\n",
    "\n",
    "First we need to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The dataset we are going to use is the [MNIST dataset](https://www.tensorflow.org/datasets/catalog/mnist), which is nicely provided by TensorFlow itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_val, y_val) = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "x_train.shape, y_train.shape, x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that it has 60000 samples for training and 10000 samples for validation.\n",
    "\n",
    "The inputs (`x`) are gray-scale images of 28 x 28 pixels. A pixel has a value between 0 and 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.min(), x_train.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels (`y`) are integers between 0 and 9 (inclusive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.min(), y_train.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.choice(len(x_val))\n",
    "x = x_val[i]\n",
    "y_true = y_val[i]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(x, cmap='gray')\n",
    "plt.title(f'label = {y_true}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a pixelated hand drawn digit. Re-run the cell again to see another random sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Now, let's create the neural network model. First we need some classes from TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to make a simple sequential model using the [`Sequential`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential) class which provides a quick way of making a stack of layers. You can simple give it a list with layers and it will stack them together into a model.\n",
    "\n",
    "The input is a 28x28 pixel image, but the simple layers we have been using so far only accept 1-dimensional samples. So, we will have to reshape the input first. This is done using the [`Flatten`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) class. Create a new instance and set the `input_shape` parameter to `(28,28)`.\n",
    "\n",
    "Then we want to add two fully connected (i.e. dense) hidden layers of **128** and **64** units. In TensorFlow this is represented by the [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) class. Create two instances with the correct number of inputs and set the `activation` parameter to `'relu'`.\n",
    "\n",
    "Finally, add one more dense layer with **10** units that represents the output layer. Set the `activation` parameter to [`'softmax'`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Softmax). This activation function will scale the outputs such that they all sum up to one. In other words, represent a probability distribution.\n",
    "\n",
    "At the end the structure is summarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "#### BEGIN IMPLEMENTATION ####\n",
    "    ...,\n",
    "    ...,\n",
    "#### END IMPLEMENTATION ####\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be similar to the following. The names of the layers might be different if you have executed the cell multiple times, and white spaces might differ a little.\n",
    "\n",
    "    Layer (type)                Output Shape              Param #   \n",
    "    =================================================================\n",
    "    flatten (Flatten)           (None, 784)               0         \n",
    "    dense (Dense)               (None, 128)               100480    \n",
    "    dense_1 (Dense)             (None, 64)                8256      \n",
    "    dense_2 (Dense)             (None, 10)                650       \n",
    "    =================================================================\n",
    "    Total params: 109386 (427.29 KB)\n",
    "    Trainable params: 109386 (427.29 KB)\n",
    "    Non-trainable params: 0 (0.00 Byte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the model we have to specify the loss function and the optimizer. This can be done with the `compile` function of the model.\n",
    "\n",
    "This time we will use the sparse categorical cross entropy loss function ([`'sparse_categorical_crossentropy'`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy)) instead of the binary cross entropy loss function we have used before. This function computes the loss for multiple classes (categorical) and for simplicity accepts integers as ground truth, hence the name 'sparse'. The loss function is specified with the `loss` parameter and it accepts string values as well as objects. Use the given string value in this case.\n",
    "\n",
    "The [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) optimizer (`'adam'`) performs better than the gradient descent method that we have implemented ourselves. This optimizer uses a mechanism called momentum to smoothen the operation. It is the de-facto standard optimizer used today. The optimizer can be specified with the `optimizer` parameter and it also accepts a string or object value.\n",
    "\n",
    "Finally, the metrics parameter can be set to an array of strings of all the [metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics) that should be measured throughout the training process. Set the `metrics` parameter to an array of at least the `'accuracy'` metric.\n",
    "\n",
    "Call the `compile` method of the model and set the parameters `loss`, `optimizer`,  and `metrics` accordingly. No ouput is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### BEGIN IMPLEMENTATION ####\n",
    "...\n",
    "#### END IMPLEMENTATION ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Time to train the network. We give it the images as input and the labels as target and let it run for 10 epochs.\n",
    "\n",
    "The `fit` function of the model will perform the forward propagation, compute the loss, run the backward propagation to get the gradients of the weights, and then call the optimizer to change the parameters accordingly. In other words, the whole training loop we have made ourselves before. It also automatically chops up the input set into batches.\n",
    "\n",
    "Call the `fit` function of the model. The first two parameters should be the X and Y arrays of the training set to serve as input and ground truth for training. Set the `epochs` parameter to `10`. The fit function can also accept an additional tuple of arrays (`(x, y)`), that can be used to validate the model every epoch. Set the `validation_data` parameter to use the validation dataset. The function will return a result structure that is needed later, so make sure to store the result in the `train_result` variable.\n",
    "\n",
    "When you run this cell a progress bar will be displayed for every epoch, showing the loss and accuracy of the model on the training set during the epoch, as well as the loss and accuracy of the model on the validation set at the end of the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### BEGIN IMPLEMENTATION ####\n",
    "train_result = ...\n",
    "#### END IMPLEMENTATION ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result structure returned by the `fit` method contains a lot of information. For instnace, it contains a history of the training and validation loss for each epoch. It represents the same data as was logged above. Let's plot this to see the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.set_title('loss')\n",
    "ax1.plot(train_result.history['loss'], label='training')\n",
    "ax1.plot(train_result.history['val_loss'], label='validation')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.set_title('accuracy')\n",
    "ax2.plot(train_result.history['accuracy'], label='training')\n",
    "ax2.plot(train_result.history['val_accuracy'], label='validation')\n",
    "ax2.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the loss went down quickly during the first epoch and the accuracy was already quite high after that (>90%). The loss only went down a little more during the remaining epochs and the accuracy increased a little as well.\n",
    "\n",
    "Notice that the accuracy is higher than what we achieved in our own implementation (~98% vs ~92%). This is mainly due to the different optimizer used. The Adam optimizer performs better in most cases than the stochastic gradient descent method we implemented.\n",
    "\n",
    "Time to evaluate the model manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Let's take a random sample from the validation set and see how well it performs.\n",
    "\n",
    "Let's first create a simple helper function that displays the input image, predict the label using the model, display the probabilities of all categories and finally shows if the prediction was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_and_predict(i):\n",
    "    x = x_val[[i]]\n",
    "    y_true = y_val[i]\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(x[0], cmap='gray')\n",
    "    plt.title(f'label = {str(y_true)}')\n",
    "    plt.show()\n",
    "\n",
    "    y_preds = model(x)[0]\n",
    "    for i, p in enumerate(y_preds):\n",
    "        print(f'#{i}: {p:.1f}')\n",
    "    y_pred = np.argmax(y_preds)\n",
    "    ok = 'OK' if y_pred == y_true else 'NOT OK'\n",
    "    print(f'prediction {y_pred} => {ok}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see a random sample of the validation set. It should predict it correctly in most cases. Now and then it will give the wrong result. Run the cell multiple times to see multiple samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = np.random.choice(len(x_val))\n",
    "print(i)\n",
    "show_and_predict(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some cases are not so very clear, even for humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_and_predict(4248) # a 2 that is not so clearly drawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_and_predict(4879) # an 'open' 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_and_predict(4443) # a 3 that is not so clearly drawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_and_predict(5176) # an 8 that could be a 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End\n",
    "\n",
    "This wraps up the end of these exercises.\n",
    "\n",
    "This last one showed that it is quite easy to create a neural network model using a framework like TensorFlow. Surprisingly, the model does quite well in such a short time of training. However, we have to admit that this dataset is quite simple; the images are (very) small and nicely pre-processed to make the digits clearly visible. For larger and more complex datasets, like ImageNet, these small neural networks will not suffice anymore. That is why it is called *Deep Learning*. The power of neural networks begins to show when the models become larger and deeper.\n",
    "\n",
    "Have fun learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
