{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siouxdnn import load_data\n",
    "X_train, Y_train, X_val, Y_val = load_data()\n",
    "print('training set', X_train.shape, Y_train.shape)\n",
    "print('validation set', X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siouxdnn import Model, reset_seed\n",
    "reset_seed()\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_before = model.compute_loss(X_val, Y_val)\n",
    "loss_before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change `w2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.w2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-9\n",
    "\n",
    "t = model.w2[13,37]\n",
    "model.w2[13,37] = t + epsilon\n",
    "loss_w2 = model.compute_loss(X_val, Y_val)\n",
    "model.w2[13,37] = t\n",
    "\n",
    "loss_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_w2 - loss_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = (loss_w2 - loss_before) / epsilon\n",
    "gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.w2[13,37] -= gradient\n",
    "\n",
    "loss_after = model.compute_loss(X_val, Y_val)\n",
    "print(f'before: {loss_before}')\n",
    "print(f'after:  {loss_after}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_after - loss_before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop():\n",
    "    initial_loss = model.compute_loss(X_val, Y_val)\n",
    "    \n",
    "    dw2 = np.zeros_like(model.w2)\n",
    "    for i in range(model.w2.shape[0]):\n",
    "        for j in range(model.w2.shape[1]):\n",
    "            t = model.w2[i, j]\n",
    "            model.w2[i, j] = t + epsilon\n",
    "            loss = model.compute_loss(X_val, Y_val)\n",
    "            model.w2[i, j] = t\n",
    "            dw2[i, j] = (loss - initial_loss) / epsilon\n",
    "\n",
    "    return initial_loss, dw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for _ in range(20):\n",
    "    loss, dw2 = backprop()\n",
    "    losses.append(loss)\n",
    "    print(loss)\n",
    "    model.w2 -= 0.1 * dw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
