{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #4 - Training\n",
    "\n",
    "Finally, we are going to train our network so it can actually predict something useful.\n",
    "\n",
    "You will implement the training loop that updates the parameters and let it run for a while.\n",
    "\n",
    "Let's start with importing the basic necessities. We will (later) import reference implementations of the functions you implemented in the previous exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training intuition\n",
    "\n",
    "We can get some intuition on how training will work by only using the function `binary_cross_entropy_loss_backward`. This is will return the gradient of the loss with respect to the predictions. So if we change the predictions themselves slightly in the opposite direction, then loss should go down and the predictions will be closer to the ground truth.\n",
    "\n",
    "Let's test this. We compute (and print) the loss at the start, then we do a number of small steps in the opposite direction of the gradients, followed by a final loss computation. The result should be that the loss ends up close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siouxdnn import binary_cross_entropy_loss, binary_cross_entropy_loss_backward\n",
    "\n",
    "y_true = np.array([[1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0]])\n",
    "y_pred = np.array([[0.6], [0.4], [0.2], [0.7], [0.1], [0.2], [0.5], [0.9], [0.8], [0.6]])\n",
    "\n",
    "print(f'loss at start {binary_cross_entropy_loss(y_true, y_pred)}')\n",
    "dl_dy_pred = binary_cross_entropy_loss_backward(y_true, y_pred)\n",
    "\n",
    "for _ in range(500):\n",
    "    y_pred = y_pred - 1e-2*dl_dy_pred\n",
    "    dl_dy_pred = binary_cross_entropy_loss_backward(y_true, y_pred)\n",
    "\n",
    "print(f'loss at end {binary_cross_entropy_loss(y_true, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training of the whole neural network will work in a similar fashion. Instead of changing the predictions we will then change the parameters `w` and `b` of each layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "We have all the building blocks available. The forward pass (also called inference) is implemented, so we can predict values. The backward propagation is implemented, so we can compute the gradients of the loss with respect to the parameters. We are finally ready to actually train the model!\n",
    "\n",
    "We will train the model on the same dataset as before. This time we will use both the training and the validation set. So let's import that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siouxdnn import load_data\n",
    "X_train, Y_train, X_val, Y_val = load_data()\n",
    "print('training set', X_train.shape, Y_train.shape)\n",
    "print('validation set', X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement the training loop. This is actually pretty straight forward. We first compute the gradients using the function you just implemented. Then you have to update all the parameters in the opposite direction with the `learning_rate` as a factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y_true, learning_rate):\n",
    "    loss, dl_dx, dl_dw1, dl_db1, dl_dw2, dl_db2, dl_dw3, dl_db3 = model.get_gradients(x, y_true)\n",
    "    #### BEGIN IMPLEMENTATION ####\n",
    "    model.w1 = ...\n",
    "    model.b1 = ...\n",
    "    model.w2 = ...\n",
    "    model.b2 = ...\n",
    "    model.w3 = ...\n",
    "    model.b3 = ...\n",
    "    #### END IMPLEMENTATION ####\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, this is it. Time to train your model. Before the loop starts a new model is created (with a fixed random seed).\n",
    "\n",
    "You have to add the main part of the loop. First train the loop one step on the training dataset using the just implemented `train` function. Then call the `evaluate` function of `model` to compute the loss on the validation set. The loop will do a 1000 training steps, each time using the whole set at once (i.e. no batches).\n",
    "\n",
    "The training set is used to actually train the network on, so the weights will be adjusted according to that input and ground truth. The validation set will be used to evaluate the performance of the network on data it hasn't \"seen\" yet.\n",
    "\n",
    "A few functions are imported and used to display a nice plot after training. You should see both the training loss and the validation loss going down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss_plot import init_loss_plot, add_loss_to_plot, finish_loss_plot\n",
    "\n",
    "from siouxdnn import Model, reset_seed\n",
    "reset_seed(123)\n",
    "model = Model()\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "init_loss_plot()\n",
    "for epoch in range(1000):\n",
    "    #### BEGIN IMPLEMENTATION ####\n",
    "    # train the model one step on the training set\n",
    "    train_loss = ...\n",
    "    # evaluate the model on the validation set\n",
    "    val_loss = ...\n",
    "    #### END IMPLEMENTATION ####\n",
    "\n",
    "    add_loss_to_plot(train_loss, val_loss)\n",
    "finish_loss_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "The model is now trained!\n",
    "\n",
    "You should see that the loss on the training set has gone down, but so did the loss on the validation set. Even data it was not trained on can be processed quite effectively.\n",
    "\n",
    "We can also take a look at the binary accuracy of the model. This computes how many of the predictions are correct, i.e. predicted > 0.5 if the ground truth is 1 and < 0.5 if the ground truth is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siouxdnn import get_accuracy\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "accuracy = get_accuracy(Y_val, y_pred)\n",
    "print(f'accuracy {accuracy:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be `accuracy 92.2%`.\n",
    "\n",
    "As you can see we do pretty well on data that was not seen yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We are done. We have written a complete neural network (of 3 layers) all from scratch and trained it on a dataset. And it even performs quite well. Congratulations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
