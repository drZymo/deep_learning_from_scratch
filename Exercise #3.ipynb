{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise #3 - Backpropagation and training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are going to train our network so it can actually predict something useful.\n",
    "\n",
    "You will first implement the whole backpropagation chain so we have the gradients of the loss with respect to the parameters of the neural network. Then we will implement the training loop and let it run for a while.\n",
    "\n",
    "Let's start with the basic necessities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from siouxdnn import dense, relu, sigmoid, binary_cross_entropy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "Since we are going from the back to the front of the network we start with the last block, the loss function. The loss function computes the loss based on the predictions of the network. So when we go back through the network we have to compute the partial derivative of the loss value with respect to those predictions. This is defined as follows.\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{\\hat{y}}} = \\frac{1}{M} \\frac{\\hat{y}-y}{\\hat{y}(1-\\hat{y})}$$\n",
    "\n",
    "It's actually already a \"chained\" function. The $\\frac{1}{M}$ part is to compensate for the `mean` used in the loss function, i.e. every sample only contributes for $\\frac{1}{M}$th to the loss. The second part is the actual partial derivative of the loss function.\n",
    "\n",
    "Implement this in the following function.\n",
    "\n",
    "By the way, the clipping of `y_pred` is done to prevent division by zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss_backward(y_true, y_pred):\n",
    "    y_pred = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "    m = y_pred.shape[0]\n",
    "    #### BEGIN IMPLEMENTATION ####\n",
    "    dy_pred = \n",
    "    #### END IMPLEMENTATION ####\n",
    "    return dy_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true = np.array([[1.0], [1.0], [0.0], [1.0], [0.0], [0.0], [0.0], [1.0], [1.0], [0.0]])\n",
    "y_pred = np.array([[0.6], [0.4], [0.2], [0.7], [0.1], [0.2], [0.5], [0.9], [0.8], [0.6]])\n",
    "dy_pred = binary_cross_entropy_loss_backward(y_true, y_pred)\n",
    "print(dy_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you implemented it correctly the output should be:\n",
    "\n",
    "    [[-0.16666667]\n",
    "     [-0.25      ]\n",
    "     [ 0.125     ]\n",
    "     [-0.14285714]\n",
    "     [ 0.11111111]\n",
    "     [ 0.125     ]\n",
    "     [ 0.2       ]\n",
    "     [-0.11111111]\n",
    "     [-0.125     ]\n",
    "     [ 0.25      ]]\n",
    "\n",
    "Otherwise, check your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this single function we can already get some intuition on how training will work. The function `binary_cross_entropy_loss_backward` will return the gradient of the predictions. So if we change the predictions themselves slightly in the opposite direction, then the predictions will be closed to the ground truth.\n",
    "\n",
    "Let's test this. We compute (and print) the loss at the start, then we do a number of small steps in the opposite direction of the gradient, followed by a final loss computation. The result should be that the loss ends up close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siouxdnn import binary_cross_entropy_loss\n",
    "\n",
    "print(f'loss at start {binary_cross_entropy_loss(y_true, y_pred)}')\n",
    "dy_pred = binary_cross_entropy_loss_backward(y_true, y_pred)\n",
    "\n",
    "for _ in range(500):\n",
    "    y_pred = y_pred - 1e-2*dy_pred\n",
    "    dy_pred = binary_cross_entropy_loss_backward(y_true, y_pred)\n",
    "\n",
    "print(f'loss at end {binary_cross_entropy_loss(y_true, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final training of the network will work in a similar fashion. Instead of changing the predictions we will then change the parameters `w` and `b` of each layer. But we need a few other building blocks before we can do that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions\n",
    "\n",
    "Next up are the activation functions. These were defined as follows.\n",
    "\n",
    "$$a_n = g(z_n)$$\n",
    "\n",
    "During backpropagation we want to compute the following chain.\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{z_n}} = \\frac{\\partial{L}}{\\partial{a_n}} \\frac{\\partial{a_n}}{\\partial{z_n}} = \\frac{\\partial{L}}{\\partial{a_n}} \\frac{\\partial{g(z_n)}}{\\partial{z_n}}$$\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "Let's begin with backpropagating the `sigmoid` function. The sigmoid function has a nice feature that it's derivative can be described in terms of its own.\n",
    "\n",
    "$$\\frac{\\partial{g(ùëß_n)}}{\\partial{z_n}} = \\frac{\\partial{ùë†ùëñùëî(ùëß_n)}}{\\partial{z_n}} = ùë†ùëñùëî(ùëß_n)(1‚àíùë†ùëñùëî(ùëß_n)) = a_n (1-a_n)$$\n",
    "\n",
    "Implement it in the function below. Let it return the output of the whole chain, i.e. $\\frac{\\partial{L}}{\\partial{z_n}}$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(z, da):\n",
    "    #### BEGIN IMPLEMENTATION ####\n",
    "    a = \n",
    "    dz = \n",
    "    #### END IMPLEMENTATION ####\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array([[ 0.90145683, 0.35448664, 0.12282405, 0.14473946],\n",
    "              [ 0.90185776,-0.83611207,-0.74306445, 0.75256026],\n",
    "              [-0.68165728, 0.46613136, 0.22696806,-0.34379635]])\n",
    "da = np.array([[ 0.09298582,-0.13436976, 0.02718292, 0.02473067],\n",
    "               [-0.12145152, 0.17550431,-0.03550441,-0.03230145],\n",
    "               [-0.15272906, 0.22070212,-0.0446479 ,-0.04062008]])\n",
    "dz = sigmoid_backward(z, da)\n",
    "print(dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be equal to:\n",
    "\n",
    "    [[ 0.01909686 -0.03255884  0.00677016  0.0061504 ]\n",
    "     [-0.02493875  0.03702021 -0.0077554  -0.00703186]\n",
    "     [-0.03406903  0.0522837  -0.01101945 -0.00986076]]\n",
    "\n",
    "\n",
    "If not, check your implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU\n",
    "\n",
    "The other activation function, ReLU, is even simpler. The ReLU function returns the input value `z` if it is greater than 0 otherwise it returns 0. That means that the gradient is 1 if `z` is greater than 0, otherwise it is 0.\n",
    "\n",
    "$$\\frac{\\partial{g(ùëß_n)}}{\\partial{z_n}} = \\frac{\\partial{relu(ùëß_n)}}{\\partial{z_n}} = \\begin{cases} 1, & z_n \\gt 0 \\\\ 0, & z_n \\le 0 \\end{cases}$$\n",
    "\n",
    "There are a few ways to compute the result. You could convert the z array to 1s and 0s using boolean logic, or you could set the `da` array values to 0 where applicable. Make sure you return a new array and not an in-place changed version of `da`, otherwise it might screw up other computations later on (see [np.copy](https://docs.scipy.org/doc/numpy-1.16.1/reference/generated/numpy.copy.html)).\n",
    "\n",
    "Remember that this function also has to return the complete chain result $\\frac{\\partial{L}}{\\partial{z_n}}$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(z, da):\n",
    "    #### BEGIN IMPLEMENTATION ####\n",
    "    dz = \n",
    "    #### END IMPLEMENTATION ####\n",
    "    return dz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing, testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array([[-0.3841685 , 0.96313277, 0.01752332, 0.70375869],\n",
    "              [-0.90051653, 0.36881432,-0.87937234,-0.07766394],\n",
    "              [ 0.99909255,-0.28151873, 0.21100903, 0.96628448]])\n",
    "da = np.array([[ 0.10135023,-0.09567351, 0.00798541, 0.01288583],\n",
    "               [ 0.12338271,-0.11647193, 0.00972135, 0.01568707],\n",
    "               [ 0.18808731,-0.17755236, 0.01481944, 0.02391371]])\n",
    "dz = relu_backward(z, da)\n",
    "print(dz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be:\n",
    "\n",
    "    [[ 0.         -0.09567351  0.00798541  0.01288583]\n",
    "     [ 0.         -0.11647193  0.          0.        ]\n",
    "     [ 0.18808731  0.          0.01481944  0.02391371]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense\n",
    "\n",
    "And last but not least, the backpropagation of the `dense` function. This function is more involved, because it uses the parameters `w` and `b` and we are actually interested in the partial derivatives of those.\n",
    "\n",
    "So instead of computing only one derivate this function should return 3.\n",
    "\n",
    "One for the output of the previous layer, so we can continue the chain backwards.\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{a_{n-1}}} = \\frac{\\partial{L}}{\\partial{z_n}} \\frac{\\partial{z_n}}{\\partial{a_{n-1}}} = \\frac{\\partial{L}}{\\partial{z_n}} w_{n}$$\n",
    "\n",
    "And two for the parameters.\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{w_n}} = \\frac{\\partial{L}}{\\partial{z_n}} \\frac{\\partial{z_n}}{\\partial{w_n}} = \\frac{\\partial{L}}{\\partial{z_n}} a_{n-1}$$\n",
    "\n",
    "$$\\frac{\\partial{L}}{\\partial{b_n}} = \\frac{\\partial{L}}{\\partial{z_n}} \\frac{\\partial{z_n}}{\\partial{b_n}} = \\frac{\\partial{L}}{\\partial{z_n}} 1 = \\frac{\\partial{L}}{\\partial{z_n}}$$\n",
    "\n",
    "Thanks to backpropagation the term $\\frac{\\partial{L}}{\\partial{z_n}}$ is already computed and given as input `dz`. Implement the rest in the function below.\n",
    "\n",
    "Remember that the shapes of the outputs must match the shapes of the inputs. So `dw` must have the same shape as `w`, `db` the same as `b`, and `da_prev` the same as `a_prev`. Take real good care of this when you multiply matrices.\n",
    "\n",
    "For instance, `dz` has shape `(#samples, #outputs)` and `w` has shape `(#inputs, #outputs)`. The output, `da_prev` must have shape `(#samples, #inputs)`. You cannot simply multiply `dz` with `w`, because then you would get `(#samples, #outputs)(#inputs, #outputs)` which is not valid. To correctly do this you have to transpose the matrix `w` (e.g. use `w.T`). Then you would get `(#samples, #outputs)(#outputs, #inputs)` which is valid and will return a matrix of shape `(#samples, #inputs)`.\n",
    "\n",
    "Finally you have to do something smart with the gradient of `b`. Remember that `b` is added to each sample via the automatic 'broadcast'. For instance, if you have 20 samples and your layer has 6 outputs, then the result of $a_{n-1} w_n$ is a tensor with shape `(20, 6)`. The `b` parameter has shape `(1, 6)` and is broadcasted (i.e. repeated) to the shape `(20, 6)` before it is added. Since $\\frac{\\partial{L}}{\\partial{b_n}}$ is equal to $\\frac{\\partial{L}}{\\partial{z_n}}$ you have to \"un-broadcast\" it to get the right shape. In this case you have to use [np.sum](https://docs.scipy.org/doc/numpy-1.16.1/reference/generated/numpy.sum.html) with the right `axis` parameter and `keepdims=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dense_backward(a_prev, dz, w):\n",
    "    #### BEGIN IMPLEMENTATION ####\n",
    "    dw = \n",
    "    db = \n",
    "    da_prev = \n",
    "    #### END IMPLEMENTATION ####\n",
    "    return dw, db, da_prev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it. This one is big, because the dense layer uses the paramters `w` and `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_prev = np.array([[ 0.35375751,-0.3776729 , 0.32942281,-0.27827347],\n",
    " [-0.93693319,-0.085921  , 0.01547009, 0.80320843],\n",
    " [-0.74478941, 0.22313203,-0.21071294, 0.86850543]])\n",
    "dz = np.array([[ 0.        , 0.03582932, 0.        , 0.05249304,-0.08647048],\n",
    " [ 0.04884253, 0.06242368, 0.11415199, 0.09145607, 0.        ],\n",
    " [ 0.04169974, 0.05329477, 0.09745826, 0.07808142, 0.        ]])\n",
    "w = np.array([[-0.89889975,-0.47854249,-0.52593611,-0.90020105, 0.63363267],\n",
    " [ 0.14985559,-0.36020986,-0.85147906,-0.68566762, 0.1925108 ],\n",
    " [-0.54756299, 0.80574929,-0.81764706,-0.4837164 , 0.66329728],\n",
    " [-0.6358417 ,-0.13549578,-0.08654791, 0.03401955,-0.01993571]])\n",
    "dw, db, da_prev = dense_backward(a_prev, dz, w)\n",
    "print(dw)\n",
    "print(db)\n",
    "print(da_prev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be:\n",
    "\n",
    "    [[-0.07681971 -0.08550531 -0.17953867 -0.12527264 -0.03058958]\n",
    "     [ 0.00510795 -0.0070035   0.01193801 -0.01026073  0.03265756]\n",
    "     [-0.00803108  0.0015388  -0.01876978  0.00225447 -0.02848535]\n",
    "     [ 0.07544718  0.08645567  0.17633087  0.12666501  0.02406244]]\n",
    "    [[ 0.09054227  0.15154777  0.21161025  0.22203054 -0.08647048]]\n",
    "    [[-0.11919067 -0.06554536 -0.05387793 -0.00134508]\n",
    "     [-0.21614243 -0.1750728  -0.11402137 -0.04628258]\n",
    "     [-0.18453349 -0.14946993 -0.09734674 -0.03951416]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the building blocks available we can build the whole backpropagation chain for our model.\n",
    "\n",
    "Below is the code of the model as it was implemented in the previous exercises. You need to implement the new `get_gradients` function.\n",
    "\n",
    "In this function, first the forward pass is computed, so all the intermediate results are available. Then the backpropagation starts. Use the right builing blocks you implemented above with the right parameters.\n",
    "\n",
    "You're almost done. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        N0, N1, N2,N3 = 5, 64, 64, 1\n",
    "        self.w1 = np.random.uniform(-0.3, 0.3, size=(N0, N1))\n",
    "        self.b1 = np.zeros((1, N1))\n",
    "        self.w2 = np.random.uniform(-0.2, 0.2, size=(N1, N2))\n",
    "        self.b2 = np.zeros((1, N2))\n",
    "        self.w3 = np.random.uniform(-0.3, 0.3, size=(N2, N3))\n",
    "        self.b3 = np.zeros((1, N3))\n",
    "        \n",
    "    def predict(self, x):\n",
    "        a0 = x\n",
    "        z1 = dense(a0, self.w1, self.b1)\n",
    "        a1 = relu(z1)\n",
    "        z2 = dense(a1, self.w2, self.b2)\n",
    "        a2 = relu(z2)\n",
    "        z3 = dense(a2, self.w3, self.b3)\n",
    "        a3 = sigmoid(z3)\n",
    "        y_pred = a3\n",
    "        return y_pred\n",
    "\n",
    "    def compute_loss(self, x, y_true):\n",
    "        y_pred = self.predict(x)\n",
    "        return binary_cross_entropy_loss(y_true, y_pred)\n",
    "\n",
    "    def get_gradients(self, x, y_true):\n",
    "        a0 = x\n",
    "\n",
    "        z1 = dense(a0, self.w1, self.b1)\n",
    "        a1 = relu(z1)\n",
    "        z2 = dense(a1, self.w2, self.b2)\n",
    "        a2 = relu(z2)\n",
    "        z3 = dense(a2, self.w3, self.b3)\n",
    "        a3 = sigmoid(z3)\n",
    "\n",
    "        y_pred = a3\n",
    "\n",
    "        loss = binary_cross_entropy_loss(y_true, y_pred)\n",
    "\n",
    "        #### BEGIN IMPLEMENTATION ####\n",
    "        dy_pred = \n",
    "\n",
    "        da3 = dy_pred\n",
    "\n",
    "        dz3 = \n",
    "        dw3, db3, da2 = \n",
    "        dz2 = \n",
    "        dw2, db2, da1 = \n",
    "        dz1 = \n",
    "        dw1, db1, da0 = \n",
    "\n",
    "        dx = da0\n",
    "        #### END IMPLEMENTATION ####\n",
    "\n",
    "        return loss, dx, dw1, db1, dw2, db2, dw3, db3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could add a test here if you implemented it right, but we need quite some data for that and we are almost done, so let's postpone that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "The model is implemented, we can predict values and compute the gradients. So we are finally ready to actually train the model. \n",
    "\n",
    "We will train the model on the same dataset as before. This time we will use both the training and the validation set. So let's import that first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siouxdnn import load_data\n",
    "X_train, Y_train, X_val, Y_val = load_data()\n",
    "print('training set', X_train.shape, Y_train.shape)\n",
    "print('validation set', X_val.shape, Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement the training loop. This is actually pretty simple. You first have to compute the gradients using the function you just implemented. Then update all the parameters in the opposite direction with the `learning_rate` as a factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x, y_true, learning_rate):\n",
    "    #### BEGIN IMPLEMENTATION ####\n",
    "    loss, dx, dw1, db1, dw2, db2, dw3, db3 = \n",
    "    model.w1 = \n",
    "    model.b1 = \n",
    "    model.w2 = \n",
    "    model.b2 = \n",
    "    model.w3 = \n",
    "    model.b3 = \n",
    "    #### END IMPLEMENTATION ####\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, this is it. Time to train your model. Run the following code and it will create a new model and run 1000 training steps, each time training on the whole training set at once (i.e. no batches).\n",
    "\n",
    "A nice plot will be displayed while training. You should see both the training loss and the validation loss going down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from loss_plot import init_loss_plot, add_loss_to_plot, finish_loss_plot\n",
    "\n",
    "from siouxdnn import reset_seed\n",
    "reset_seed(123)\n",
    "model = Model()\n",
    "\n",
    "learning_rate = 1e-2\n",
    "\n",
    "init_loss_plot()\n",
    "for epoch in range(1000):\n",
    "    train_loss = train(model, X_train, Y_train, learning_rate)\n",
    "    val_loss = model.compute_loss(X_val, Y_val)\n",
    "\n",
    "    add_loss_to_plot(train_loss, val_loss, epoch%50 == 0)\n",
    "finish_loss_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "The model is now trained!\n",
    "\n",
    "Let's check how well it performs by inspecting the metrics on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siouxdnn import get_metrics\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "accuracy, precision, recall = get_metrics(Y_val, y_pred)\n",
    "print(f'accuracy {accuracy:.3f}, precision {precision:.3f}, recall {recall:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be `accuracy 0.906, precision 0.884, recall 0.974`.\n",
    "\n",
    "As you can see we do pretty well. An accuracy of 90.6% and high precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End\n",
    "\n",
    "We are done. We have written a complete neural network (of 3 layers) all from scratch and trained it on a dataset. And it even performs quite well. Congratulations!\n",
    "\n",
    "# ü•≥"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
